{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPspRq7R/FPmCCs4J0DpDiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParisaArbab/AI--First-Order-Logic-and-Prolog/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE-9tRipl9ta",
        "outputId": "e2541523-7225-4ff1-fa3c-84962e224205"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0- Libraries**\n"
      ],
      "metadata": {
        "id": "w-UkF9aiYTKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vpxJa4lJ7gvw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1- Load Dataset**"
      ],
      "metadata": {
        "id": "OYZyKSXXYgYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ESOL dataset\n",
        "dataset = MoleculeNet(root='/tmp/ESOL', name='ESOL')"
      ],
      "metadata": {
        "id": "nEIR7rGZ7z7n"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Display Initial Node Features and Edge Attributes\n",
        "# data = toy_data[0]\n",
        "data = dataset[0] # extract one graph\n",
        "print(\"Num of nodes:\", data.x.shape[0])\n",
        "print(\"Num of Node Features:\", data.x.shape[1])\n",
        "\n",
        "print(\"Num of Edges:\", data.edge_attr.shape[0])\n",
        "print(\"Num of Edge Features:\", data.edge_attr.shape[1])\n",
        "\n",
        "data.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFiML6EV50VI",
        "outputId": "2dc4a44b-c07d-425f-f69e-e80369bcccd9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of nodes: 32\n",
            "Num of Node Features: 9\n",
            "Num of Edges: 68\n",
            "Num of Edge Features: 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2- Encode neighbors with Edge weights**"
      ],
      "metadata": {
        "id": "5QceWg9gWZFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This function adds neighbor information (e.g., degree features or adjacency-based features) to the original node features of each graph."
      ],
      "metadata": {
        "id": "YN0rNXMELkAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to encode graph with neighbor information\n",
        "def encode_neighbors_with_edge_weights(data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    num_nodes = x.size(0)\n",
        "    adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.float32, device=x.device)\n",
        "\n",
        "    for i in range(edge_index.size(1)):\n",
        "        src, dst = edge_index[:, i]\n",
        "        adj_matrix[src, dst] = 1  # Assuming unweighted edges for simplicity\n",
        "\n",
        "    # Add adjacency rows to node features\n",
        "    # neighbor_features = adj_matrix.sum(dim=1, keepdim=True)  # Example: Degree as additional feature\n",
        "    data.x = torch.cat([x, adj_matrix], dim=1)  # Concatenate original features with neighbor features\n",
        "    return data\n",
        "\n",
        "# Apply encoding to all graphs in the dataset\n",
        "#encoded_dataset = [encode_neighbors_with_edge_weights(data) for data in dataset]"
      ],
      "metadata": {
        "id": "xSKLVE-eWvXI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first graph's node features before and after encoding\n",
        "# print(\"Original Node Features (First Graph):\")\n",
        "# print(dataset[0].x.size())\n",
        "\n",
        "print(\"Data size:         \", data.x.size())\n",
        "encoded_dataset = encode_neighbors_with_edge_weights(data.clone())\n",
        "\n",
        "# print(\"\\nEncoded Node Features (First Graph):\")\n",
        "# encoded_dataset[0].x.size()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Encoded Data size: \" , encoded_dataset.x.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AybCMrJ7TD9E",
        "outputId": "84c3405f-82a4-44ca-f37a-901b72a14cd7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size:          torch.Size([32, 9])\n",
            "Encoded Data size:  torch.Size([32, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(dataset)):\n",
        "    data = dataset[i]\n",
        "for i in range(len(dataset)):\n",
        "    data = dataset[i]\n",
        "    encoded_dataset = encode_neighbors_with_edge_weights(data.clone())\n",
        "    if i % 100 == 0:\n",
        "      print('\\n', data.x.size())\n",
        "      print(encoded_dataset.x.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbQMvdZX_kU6",
        "outputId": "af8af9b8-73c5-4a56-896d-21d8765abc33"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " torch.Size([32, 9])\n",
            "torch.Size([32, 41])\n",
            "\n",
            " torch.Size([24, 9])\n",
            "torch.Size([24, 33])\n",
            "\n",
            " torch.Size([17, 9])\n",
            "torch.Size([17, 26])\n",
            "\n",
            " torch.Size([19, 9])\n",
            "torch.Size([19, 28])\n",
            "\n",
            " torch.Size([16, 9])\n",
            "torch.Size([16, 25])\n",
            "\n",
            " torch.Size([23, 9])\n",
            "torch.Size([23, 32])\n",
            "\n",
            " torch.Size([2, 9])\n",
            "torch.Size([2, 11])\n",
            "\n",
            " torch.Size([19, 9])\n",
            "torch.Size([19, 28])\n",
            "\n",
            " torch.Size([6, 9])\n",
            "torch.Size([6, 15])\n",
            "\n",
            " torch.Size([17, 9])\n",
            "torch.Size([17, 26])\n",
            "\n",
            " torch.Size([5, 9])\n",
            "torch.Size([5, 14])\n",
            "\n",
            " torch.Size([7, 9])\n",
            "torch.Size([7, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3- Padding**"
      ],
      "metadata": {
        "id": "JMa9MkBp7I_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_nodes = 0\n",
        "max_features = 0\n",
        "for i in range(len(dataset)):\n",
        "    data = dataset[i]\n",
        "    num_nodes = data.x.size(0)\n",
        "    feature_dim = data.x.size(1)\n",
        "    max_nodes = max(max_nodes, num_nodes)\n",
        "    max_features = max(max_features, feature_dim)\n",
        "\n",
        "print(f\"The largest number of nodes in the dataset is: {max_nodes}\")\n",
        "print(f\"The largest number of features in the dataset is: {max_features}\")"
      ],
      "metadata": {
        "id": "MVTgss3X8IcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0cc9eaa-6db5-4d3f-a70a-7b1edb05bcc1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest number of nodes in the dataset is: 55\n",
            "The largest number of features in the dataset is: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_x(x, max_nodes, max_features):\n",
        "    num_nodes = x.size(0)\n",
        "    num_features = x.size(1)\n",
        "\n",
        "    # Pad nodes with zero rows\n",
        "    padding_nodes = max_nodes - num_nodes\n",
        "    if padding_nodes > 0:\n",
        "      node_padding = torch.zeros(padding_nodes, num_features, dtype=data.x.dtype, device=data.x.device)\n",
        "      data.x = torch.cat([data.x, node_padding], dim=0)\n",
        "\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "bmWh0dMu7dR9"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_x(x, max_nodes, max_features):\n",
        "    num_nodes = x.size(0)\n",
        "    num_features = x.size(1)\n",
        "    padding_nodes = max_nodes - num_nodes\n",
        "    padding_features = max_features - num_features\n",
        "\n",
        "    if padding_nodes > 0:\n",
        "        padding = torch.zeros((padding_nodes, x.size(1)), device=x.device)\n",
        "        x = torch.cat([x, padding], dim=0)\n",
        "\n",
        "    if padding_features > 0:\n",
        "        padding = torch.zeros((x.size(0), padding_features), device=x.device)\n",
        "        x = torch.cat([x, padding], dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Dz8poRm48SYg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = []\n",
        "for data in dataset:\n",
        "    encoded_dataset = encode_neighbors_with_edge_weights(data.clone())\n",
        "    padded_dataset = padding_x(encoded_dataset.x, max_nodes, max_features+max_nodes)\n",
        "    padded.append(padded_dataset)\n",
        "\n",
        "padded[10].size()\n"
      ],
      "metadata": {
        "id": "N8SZsu2N8bf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ddc0c3f-6b00-4376-cc0f-aa48db92ad77"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([55, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'padded' is your list of padded data tensors as in the provided code\n",
        "# loader = DataLoader(padded, batch_size=64, shuffle=True) # Adjust batch_size as needed\n",
        "# next(iter(loader)).size() # batch_size x num_nodes x num_featurs"
      ],
      "metadata": {
        "id": "Gm-4He7hDaWI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the Data objects with padded features\n",
        "padded_data_list = []\n",
        "for i, data in enumerate(dataset):\n",
        "    encoded_dataset = encode_neighbors_with_edge_weights(data.clone())\n",
        "    padded_x = padding_x(encoded_dataset.x, max_nodes, max_features + max_nodes)\n",
        "\n",
        "    # Create a new Data object with the padded features and original attributes\n",
        "    padded_data = data.clone()  # Clone to avoid modifying the original dataset\n",
        "    padded_data.x = padded_x\n",
        "\n",
        "    padded_data_list.append(padded_data)\n",
        "\n",
        "# Create DataLoader from the list of Data objects\n",
        "loader = DataLoader(padded_data_list, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "s8clE2k3UPj-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4- Split the Dataset**"
      ],
      "metadata": {
        "id": "YhnbLHQO738x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset (80% train, 10% val, 10% test)\n",
        "train_size = int(0.8 * len(padded_data_list))\n",
        "valid_size = int(0.1 * len(padded_data_list))\n",
        "test_size = len(padded_data_list) - train_size - valid_size\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    padded_data_list, [train_size, valid_size, test_size]\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "9kVFDQl-8qZE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5- Define Models**"
      ],
      "metadata": {
        "id": "MniEN5Lw8via"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerGraphModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes=2, num_heads=8, dropout=0.1):\n",
        "        super(TransformerGraphModel, self).__init__()\n",
        "\n",
        "        # Linear layer to project input features to the required embed_dim\n",
        "        # self.input_projection = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Feedforward layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: batch_size x num_nodes x num_features (input_dim)\n",
        "\n",
        "        # Project input features to the required embed_dim\n",
        "        #x = self.input_projection(x)  # Shape: (batch_size, num_nodes, embed_dim)\n",
        "\n",
        "        # Apply multi-head self-attention\n",
        "        attn_output, _ = self.attention(x, x, x)  # Query, Key, Value are all x\n",
        "\n",
        "        # Combine attention output with input\n",
        "        x = x + attn_output  # Residual connection: batch_size x num_nodes x embed_dim\n",
        "\n",
        "        # Feedforward layers\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x  # x: batch_size x num_nodes x num_classes // assert\n",
        "\n",
        "        # # Pool node features to graph-level representation\n",
        "        # x = global_mean_pool(x, batch)  # Shape: (batch_size, embed_dim)\n",
        "\n",
        "        # # Feedforward layers\n",
        "        # x = self.fc1(x).relu()\n",
        "        # return self.fc2(x).squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #     class TransformerGraphModel(nn.Module):\n",
        "    # def __init__(self, input_dim, hidden_dim, num_classes=1, num_heads=8, dropout=0.1): # num_classes=1 for regression\n",
        "    #     super(TransformerGraphModel, self).__init__()\n",
        "    #     self.attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "    #     self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    #     self.relu = nn.ReLU()\n",
        "    #     self.fc2 = nn.Linear(hidden_dim, num_classes) # Output dim is 1 for regression\n",
        "    #     self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # def forward(self, x, batch): # Added batch to the input\n",
        "    #     attn_output, _ = self.attention(x, x, x)\n",
        "    #     x = x + attn_output\n",
        "    #     x = self.dropout(self.relu(self.fc1(x)))\n",
        "    #     x = self.fc2(x)\n",
        "    #     x = global_mean_pool(x, batch) # Apply global_mean_pool\n",
        "    #     return x.squeeze(-1) # Return a single value per graph"
      ],
      "metadata": {
        "id": "6ddQi9vQ85o7"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define GCN Model\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.fc(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "KeVTmFUfKenT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define GAT Model\n",
        "class GATModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.fc(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "TByE3EFZKlu1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6- Initialize model**"
      ],
      "metadata": {
        "id": "f-FaRyHH9ACZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Models\n",
        "input_dim = max_features+ max_nodes\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "dropout=0.1\n",
        "models = {\n",
        "    \"GCN\": GCNModel(input_dim, hidden_dim, output_dim),\n",
        "    \"GAT\": GATModel(input_dim, hidden_dim, output_dim),\n",
        "    \"TransformerGraph\": TransformerGraphModel(input_dim, hidden_dim, output_dim, num_heads, dropout),\n",
        "}\n"
      ],
      "metadata": {
        "id": "ugv__xyI9OGs"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = next(iter(loader))\n",
        "print(\"Input size:\", input.size())\n",
        "input_dim = input.size(-1)\n",
        "\n",
        "# model = TransformerGraphModel(input_dim=input_dim, hidden_dim=64)\n",
        "# output = model(input)\n",
        "# print(\"Output size:\", output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2vpQ56sE61U",
        "outputId": "6a853e7d-fa23-44b7-9e52-0cb8f0a6ecfd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input size: (3520, 3520)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset.x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nj_fXz4HL9y",
        "outputId": "a2b2f55d-36d5-4c81-faf3-842c94624f6b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.,  0.,  4.,  5.,  3.,  0.,  4.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 8.,  0.,  2.,  5.,  0.,  0.,  4.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [15.,  0.,  4.,  5.,  0.,  0.,  4.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,\n",
              "          0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 8.,  0.,  1.,  5.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 8.,  0.,  2.,  5.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
              "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  4.,  5.,  3.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 8.,  0.,  2.,  5.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
              "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [17.,  0.,  1.,  5.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  0.,  0.,  3.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  1.,  0.,  3.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  0.,  0.,  3.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
              "        [17.,  0.,  1.,  5.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  0.,  0.,  3.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.],\n",
              "        [17.,  0.,  1.,  5.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  1.,  0.,  3.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.],\n",
              "        [ 6.,  0.,  3.,  5.,  0.,  0.,  3.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.],\n",
              "        [17.,  0.,  1.,  5.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7-  Training and Evaluation Functions**"
      ],
      "metadata": {
        "id": "xdlxtd0Dxa6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Target shape: {data.y.shape}\")\n",
        "#print(f\"Target values: {data.y}\")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_model(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # The TransformerGraphModel directly uses the encoded_dataset for its input.\n",
        "        # The encoded features (data.x) are fed into the model during training and evaluation\n",
        "        # Check if the model requires `edge_index`\n",
        "        if isinstance(model, TransformerGraphModel):\n",
        "            out = model(data.x.float(),data.batch)\n",
        "        else:\n",
        "            out = model(data.x.float(), data.edge_index, data.batch)\n",
        "\n",
        "        loss = criterion(out, data.y.squeeze().float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Check if the model requires `edge_index`\n",
        "            if isinstance(model, TransformerGraphModel):\n",
        "                out = model(data.x.float(), data.batch)\n",
        "            else:\n",
        "                out = model(data.x.float(), data.edge_index, data.batch)\n",
        "\n",
        "            loss = criterion(out, data.y.squeeze().float())\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iU-qS8zxZCF9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8- Analysis**"
      ],
      "metadata": {
        "id": "nIZ-syzX0r_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    for epoch in range(20):\n",
        "        train_loss = train_model(model, train_loader, optimizer)\n",
        "        valid_loss = evaluate_model(model, valid_loader)\n",
        "        print(f\"{model_name} Epoch {epoch+1:02d}: Train Loss: {train_loss:.4f}, Validation RMSE: {valid_loss ** 0.5:.4f}\")\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "\n",
        "    test_loss = evaluate_model(model, test_loader)\n",
        "    results[model_name] = test_loss ** 0.5  # RMSE\n",
        "\n",
        "# Plot Results\n",
        "plt.bar(results.keys(), results.values(), color=['blue', 'orange', 'green'])\n",
        "plt.title('Model Comparison (RMSE)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Model')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDbNLarhxrrr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba25f6a9-b49e-41bc-95f3-5684bc9db381"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN Epoch 01: Train Loss: 10.6331, Validation RMSE: 2.9088\n",
            "GCN Epoch 02: Train Loss: 4.4472, Validation RMSE: 1.7157\n",
            "GCN Epoch 03: Train Loss: 2.7365, Validation RMSE: 1.6606\n",
            "GCN Epoch 04: Train Loss: 2.6112, Validation RMSE: 1.7236\n",
            "GCN Epoch 05: Train Loss: 2.6173, Validation RMSE: 1.6670\n",
            "GCN Epoch 06: Train Loss: 2.5625, Validation RMSE: 1.6596\n",
            "GCN Epoch 07: Train Loss: 2.4591, Validation RMSE: 1.6532\n",
            "GCN Epoch 08: Train Loss: 2.4556, Validation RMSE: 1.6388\n",
            "GCN Epoch 09: Train Loss: 2.6729, Validation RMSE: 1.6420\n",
            "GCN Epoch 10: Train Loss: 2.3912, Validation RMSE: 1.6274\n",
            "GCN Epoch 11: Train Loss: 2.6474, Validation RMSE: 1.6186\n",
            "GCN Epoch 12: Train Loss: 2.4124, Validation RMSE: 1.6161\n",
            "GCN Epoch 13: Train Loss: 2.3099, Validation RMSE: 1.6053\n",
            "GCN Epoch 14: Train Loss: 2.2793, Validation RMSE: 1.5917\n",
            "GCN Epoch 15: Train Loss: 2.3474, Validation RMSE: 1.5784\n",
            "GCN Epoch 16: Train Loss: 2.2656, Validation RMSE: 1.5779\n",
            "GCN Epoch 17: Train Loss: 2.2849, Validation RMSE: 1.5891\n",
            "GCN Epoch 18: Train Loss: 2.2677, Validation RMSE: 1.5395\n",
            "GCN Epoch 19: Train Loss: 2.1833, Validation RMSE: 1.5816\n",
            "GCN Epoch 20: Train Loss: 2.2630, Validation RMSE: 1.5460\n",
            "GAT Epoch 01: Train Loss: 8.0066, Validation RMSE: 1.7246\n",
            "GAT Epoch 02: Train Loss: 2.6670, Validation RMSE: 1.6107\n",
            "GAT Epoch 03: Train Loss: 2.6749, Validation RMSE: 1.5895\n",
            "GAT Epoch 04: Train Loss: 2.4030, Validation RMSE: 1.5856\n",
            "GAT Epoch 05: Train Loss: 2.4396, Validation RMSE: 1.5101\n",
            "GAT Epoch 06: Train Loss: 2.1429, Validation RMSE: 1.5773\n",
            "GAT Epoch 07: Train Loss: 2.0441, Validation RMSE: 1.4251\n",
            "GAT Epoch 08: Train Loss: 1.9356, Validation RMSE: 1.4095\n",
            "GAT Epoch 09: Train Loss: 1.7903, Validation RMSE: 1.3414\n",
            "GAT Epoch 10: Train Loss: 1.7032, Validation RMSE: 1.3550\n",
            "GAT Epoch 11: Train Loss: 1.5019, Validation RMSE: 1.2553\n",
            "GAT Epoch 12: Train Loss: 1.4034, Validation RMSE: 1.1644\n",
            "GAT Epoch 13: Train Loss: 1.2306, Validation RMSE: 1.1228\n",
            "GAT Epoch 14: Train Loss: 1.1974, Validation RMSE: 1.1094\n",
            "GAT Epoch 15: Train Loss: 1.1209, Validation RMSE: 1.0722\n",
            "GAT Epoch 16: Train Loss: 1.0888, Validation RMSE: 1.0285\n",
            "GAT Epoch 17: Train Loss: 1.0384, Validation RMSE: 0.9982\n",
            "GAT Epoch 18: Train Loss: 1.0625, Validation RMSE: 1.0394\n",
            "GAT Epoch 19: Train Loss: 1.0036, Validation RMSE: 1.0023\n",
            "GAT Epoch 20: Train Loss: 1.0325, Validation RMSE: 0.9811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([3520, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([330, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([48])) that is different to the input size (torch.Size([2640, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerGraph Epoch 01: Train Loss: 9.9295, Validation RMSE: 2.8847\n",
            "TransformerGraph Epoch 02: Train Loss: 5.5455, Validation RMSE: 2.2955\n",
            "TransformerGraph Epoch 03: Train Loss: 4.2650, Validation RMSE: 2.3354\n",
            "TransformerGraph Epoch 04: Train Loss: 4.4594, Validation RMSE: 2.2961\n",
            "TransformerGraph Epoch 05: Train Loss: 4.2807, Validation RMSE: 2.3311\n",
            "TransformerGraph Epoch 06: Train Loss: 4.1367, Validation RMSE: 2.3026\n",
            "TransformerGraph Epoch 07: Train Loss: 4.7600, Validation RMSE: 2.2934\n",
            "TransformerGraph Epoch 08: Train Loss: 4.0934, Validation RMSE: 2.3060\n",
            "TransformerGraph Epoch 09: Train Loss: 4.2007, Validation RMSE: 2.2922\n",
            "TransformerGraph Epoch 10: Train Loss: 4.3090, Validation RMSE: 2.3099\n",
            "TransformerGraph Epoch 11: Train Loss: 4.3508, Validation RMSE: 2.3287\n",
            "TransformerGraph Epoch 12: Train Loss: 4.4821, Validation RMSE: 2.2932\n",
            "TransformerGraph Epoch 13: Train Loss: 4.3757, Validation RMSE: 2.3139\n",
            "TransformerGraph Epoch 14: Train Loss: 4.2564, Validation RMSE: 2.2971\n",
            "TransformerGraph Epoch 15: Train Loss: 4.1410, Validation RMSE: 2.3188\n",
            "TransformerGraph Epoch 16: Train Loss: 4.2727, Validation RMSE: 2.2939\n",
            "TransformerGraph Epoch 17: Train Loss: 4.1904, Validation RMSE: 2.3029\n",
            "TransformerGraph Epoch 18: Train Loss: 4.1691, Validation RMSE: 2.2933\n",
            "TransformerGraph Epoch 19: Train Loss: 4.6452, Validation RMSE: 2.3057\n",
            "TransformerGraph Epoch 20: Train Loss: 4.1197, Validation RMSE: 2.3159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([2750, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how to fix those warning? input and output size should be: Input size: torch.Size([64, 55, 64])\n",
        "# Output size: torch.Size([64, 55, 2])\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install torch-geometric\n",
        "!pip install rdkit\n",
        "\n",
        "# **0- Libraries**\n",
        "\n",
        "# **1- Load Dataset**\n",
        "# Load ESOL dataset\n",
        "dataset = MoleculeNet(root='/tmp/ESOL', name='ESOL')\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# **5- Define Models**\n",
        "class TransformerGraphModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes=1, num_heads=8, dropout=0.1): # num_classes=1 for regression\n",
        "        super(TransformerGraphModel, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes) # Output dim is 1 for regression\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, batch): # Added batch to the input\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        x = global_mean_pool(x, batch) # Apply global_mean_pool\n",
        "        return x.squeeze(-1) # Return a single value per graph\n",
        "\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# **8- Analysis**\n",
        "# ... (rest of your code)\n",
        "\n",
        "\n",
        "def train_model(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if isinstance(model, TransformerGraphModel):\n",
        "            out = model(data.x.float(), data.batch) # Pass batch to TransformerGraphModel\n",
        "        else:\n",
        "            out = model(data.x.float(), data.edge_index, data.batch)\n",
        "\n",
        "        loss = criterion(out, data.y.squeeze().float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            if isinstance(model, TransformerGraphModel):\n",
        "                out = model(data.x.float(), data.batch) # Pass batch to TransformerGraphModel\n",
        "            else:\n",
        "                out = model(data.x.float(), data.edge_index, data.batch)\n",
        "\n",
        "            loss = criterion(out, data.y.squeeze().float())\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "qUeMoixJWrc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}